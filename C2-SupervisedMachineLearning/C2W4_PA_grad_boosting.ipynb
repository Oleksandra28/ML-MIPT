{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг своими руками\n",
    "\n",
    "**Внимание:** в тексте задания произошли изменения - поменялось число деревьев (теперь 50), правило изменения величины шага в задании 3 и добавился параметр `random_state` у решающего дерева. Правильные ответы не поменялись, но теперь их проще получить. Также исправлена опечатка в функции `gbm_predict`.\n",
    "\n",
    "В этом задании будет использоваться датасет `boston` из `sklearn.datasets`. Оставьте последние 25% объектов для контроля качества, разделив `X` и `y` на `X_train`, `y_train` и `X_test`, `y_test`.\n",
    "\n",
    "Целью задания будет реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import ensemble, model_selection, metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train_samples = X_train.shape[0]\n",
    "n_train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "Как вы уже знаете из лекций, **бустинг** - это метод построения композиций базовых алгоритмов с помощью последовательного добавления к текущей композиции нового алгоритма с некоторым коэффициентом. \n",
    "\n",
    "Градиентный бустинг обучает каждый новый алгоритм так, чтобы он приближал антиградиент ошибки по ответам композиции на обучающей выборке. Аналогично минимизации функций методом градиентного спуска, в градиентном бустинге мы подправляем композицию, изменяя алгоритм в направлении антиградиента ошибки.\n",
    "\n",
    "Воспользуйтесь формулой из лекций, задающей ответы на обучающей выборке, на которые нужно обучать новый алгоритм (фактически это лишь чуть более подробно расписанный градиент от ошибки), и получите частный ее случай, если функция потерь `L` - квадрат отклонения ответа композиции `a(x)` от правильного ответа `y` на данном `x`.\n",
    "\n",
    "Если вы давно не считали производную самостоятельно, вам поможет таблица производных элементарных функций (которую несложно найти в интернете) и правило дифференцирования сложной функции. После дифференцирования квадрата у вас возникнет множитель 2 — т.к. нам все равно предстоит выбирать коэффициент, с которым будет добавлен новый базовый алгоритм, проигноируйте этот множитель при дальнейшем построении алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_derivative(y_true, y_pred):\n",
    "    return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbm_predict(X, base_algorithms_list, coefficients_list):\n",
    "    y_pred = [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "    return np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "Заведите массив для объектов `DecisionTreeRegressor` (будем их использовать в качестве базовых алгоритмов) и для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами). \n",
    "\n",
    "В цикле от обучите последовательно 50 решающих деревьев с параметрами `max_depth=5` и `random_state=42` (остальные параметры - по умолчанию). В бустинге зачастую используются сотни и тысячи деревьев, но мы ограничимся 50, чтобы алгоритм работал быстрее, и его было проще отлаживать (т.к. цель задания разобраться, как работает метод). Каждое дерево должно обучаться на одном и том же множестве объектов, но ответы, которые учится прогнозировать дерево, будут меняться в соответствие с полученным в задании 1 правилом. \n",
    "\n",
    "Попробуйте для начала всегда брать коэффициент равным 0.9. Обычно оправдано выбирать коэффициент значительно меньшим - порядка 0.05 или 0.1, но т.к. в нашем учебном примере на стандартном датасете будет всего 50 деревьев, возьмем для начала шаг побольше.\n",
    "\n",
    "В процессе реализации обучения вам потребуется функция, которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке `X`:\n",
    "\n",
    "```\n",
    "def gbm_predict(X):\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "(считаем, что base_algorithms_list - список с базовыми алгоритмами, coefficients_list - список с коэффициентами перед алгоритмами)\n",
    "```\n",
    "\n",
    "Эта же функция поможет вам получить прогноз на контрольной выборке и оценить качество работы вашего алгоритма с помощью `mean_squared_error` в `sklearn.metrics`. \n",
    "\n",
    "Возведите результат в степень 0.5, чтобы получить `RMSE`. Полученное значение `RMSE` — **ответ в пункте 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 50\n",
    "max_depth = 5\n",
    "random_state = 42\n",
    "\n",
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "predictions_list = []\n",
    "mse_errors_list = []\n",
    "mse_sklearn_errors_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[24.102      20.48068966 31.8675     31.8675     31.8675    ]\n",
      "[24.0102     22.19925484 36.56328605 33.34403745 34.93779392]\n",
      "[24.34648444 21.63489343 35.99892464 32.77967604 34.37343251]\n",
      "[24.69302823 21.98143722 36.0620521  33.12621983 34.7199763 ]\n",
      "[24.78287823 21.52195299 36.1519021  33.21606982 34.80982629]\n",
      "[24.15934048 21.30908401 35.43302472 33.44058951 35.03434598]\n",
      "[24.64267153 21.35941326 35.48335397 33.49091876 35.08467523]\n",
      "[24.5233785  21.24012023 35.36406095 33.28995694 34.9653822 ]\n",
      "[24.55364066 21.27038239 35.3943231  33.3202191  34.99564436]\n",
      "[24.36175334 21.2392808  35.36322151 33.27864951 36.07956444]\n",
      "[23.95303868 21.59174916 35.35521432 33.27064232 36.43203279]\n",
      "[23.97455557 21.61326605 35.37673121 33.29215921 36.45354969]\n",
      "[23.98390733 21.62261781 34.76767312 33.2374804  36.46290145]\n",
      "[23.9934274  21.63213789 34.7771932  33.24700047 36.47242152]\n",
      "[23.98152215 21.62023264 34.76528795 33.23509523 36.46051627]\n",
      "[23.99284406 21.58174837 34.72680368 33.30382294 36.46147009]\n",
      "[23.9362771  21.52518141 34.80781979 33.38483905 36.28370153]\n",
      "[23.87630068 21.46520499 34.74784337 33.37966089 36.27852337]\n",
      "[23.98763007 21.48464859 34.76728697 33.34895088 36.24781336]\n",
      "[24.0805347  21.50493312 34.7875715  33.3692354  36.26809788]\n",
      "[24.08901295 21.51341136 34.79604974 33.34945896 36.24832144]\n",
      "[24.09028482 21.51468323 34.79732161 33.35073083 36.24959331]\n",
      "[24.05112737 21.57662142 34.79589533 33.34329274 36.24215522]\n",
      "[23.98974915 21.57544659 34.7947205  33.34211791 36.24866412]\n",
      "[23.98513428 21.56622115 34.69769486 33.34877751 36.15163848]\n",
      "[23.98737987 21.56846675 34.69994046 33.35102311 36.15388407]\n",
      "[24.04059399 21.56969258 34.70089169 33.39510231 36.15483531]\n",
      "[24.0040594  21.57035587 34.70155498 33.3957656  36.1554986 ]\n",
      "[24.00359996 21.56989643 34.70109554 33.39530616 36.15503916]\n",
      "[24.00872134 21.57501781 34.70621692 33.40042754 36.16016054]\n",
      "[24.00721437 21.57351085 34.70470996 33.39892058 36.15865357]\n",
      "[24.01187442 21.5781709  34.70145785 33.39566847 36.15869453]\n",
      "[24.01140639 21.57770286 34.70098982 33.39520044 36.15822649]\n",
      "[24.00240475 21.57348748 34.69677444 33.3861988  36.19043153]\n",
      "[23.99510211 21.58000261 34.70328957 33.39271392 36.19114892]\n",
      "[23.9933268  21.5782273  34.70541143 33.39483578 36.18937361]\n",
      "[23.9921481  21.5770486  34.70423272 33.39365708 36.18819491]\n",
      "[23.99422452 21.58482688 34.70630914 33.3957335  36.19027133]\n",
      "[23.99527247 21.57217707 34.7073571  33.39678146 36.19131929]\n",
      "[23.99467725 21.57158184 34.70676187 33.39618623 36.19072406]\n",
      "[23.9970086  21.58387382 34.70909322 33.39563161 36.19016944]\n",
      "[23.99755271 21.58441792 34.70742607 33.40134473 36.18880886]\n",
      "[23.99300643 21.58559426 34.7086024  33.40252106 36.1899852 ]\n",
      "[23.99540214 21.58604352 34.70703655 33.40287699 36.19849072]\n",
      "[23.99529801 21.58617457 34.70693242 33.40277286 36.19838659]\n",
      "[23.99390962 21.59861746 34.70256916 33.40175822 36.19737195]\n",
      "[23.99447795 21.59918579 34.70186291 33.40232655 36.19794028]\n",
      "[23.99944779 21.59796907 34.70064619 33.40110983 36.19657982]\n",
      "[24.00045409 21.59897536 34.70165248 33.40211612 36.19758611]\n"
     ]
    }
   ],
   "source": [
    "current_prediction = np.asarray([0]*n_train_samples)\n",
    "for i in range(n_trees):\n",
    "    current_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=random_state)\n",
    "    current_coefficient = 0.9\n",
    "        \n",
    "    residuals = loss_derivative(y_train, current_prediction)\n",
    "    print(current_prediction[:5])\n",
    "    current_tree.fit(X_train, residuals)    \n",
    "    \n",
    "    base_algorithms_list.append(current_tree)\n",
    "    coefficients_list.append(current_coefficient)\n",
    "\n",
    "    current_prediction = gbm_predict(X_train, base_algorithms_list, coefficients_list)\n",
    "    \n",
    "    predictions_list.append(current_prediction)\n",
    "    error = loss(y_train, current_prediction)\n",
    "    mse_errors_list.append(error)\n",
    "    current_error = mean_squared_error(y_train, current_prediction)\n",
    "    mse_sklearn_errors_list.append(current_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.331605457099899"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_sklearn_errors_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.331605457099899"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_errors_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prediction = gbm_predict(X_test, base_algorithms_list, coefficients_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.07851444, 15.48786565, 14.13609346])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_prediction[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.2, 10.4, 10.9])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_error = mean_squared_error(y_test, current_prediction, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.455623403859612"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.763826724740735"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, current_prediction, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Вас может также беспокоить, что двигаясь с постоянным шагом, вблизи минимума ошибки ответы на обучающей выборке меняются слишком резко, перескакивая через минимум. \n",
    "\n",
    "Попробуйте уменьшать вес перед каждым алгоритмом с каждой следующей итерацией по формуле `0.9 / (1.0 + i)`, где `i` - номер итерации (от 0 до 49). Используйте качество работы алгоритма как **ответ в пункте 3**. \n",
    "\n",
    "В реальности часто применяется следующая стратегия выбора шага: как только выбран алгоритм, подберем коэффициент перед ним численным методом оптимизации таким образом, чтобы отклонение от правильных ответов было минимальным. Мы не будем предлагать вам реализовать это для выполнения задания, но рекомендуем попробовать разобраться с такой стратегией и реализовать ее при случае для себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 50\n",
    "max_depth = 5\n",
    "random_state = 42\n",
    "\n",
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "predictions_list = []\n",
    "mse_errors_list = []\n",
    "mse_sklearn_errors_list = []\n",
    "\n",
    "current_prediction = np.asarray([0]*n_train_samples)\n",
    "for i in range(n_trees):\n",
    "    current_tree = DecisionTreeRegressor(max_depth=max_depth, random_state=random_state)\n",
    "    current_coefficient = 0.9/(1.0 + i)\n",
    "        \n",
    "    residuals = loss_derivative(y_train, current_prediction)\n",
    "    current_tree.fit(X_train, residuals)    \n",
    "    \n",
    "    base_algorithms_list.append(current_tree)\n",
    "    coefficients_list.append(current_coefficient)\n",
    "\n",
    "    current_prediction = gbm_predict(X_train, base_algorithms_list, coefficients_list)\n",
    "    \n",
    "    predictions_list.append(current_prediction)\n",
    "    \n",
    "    error = loss(y_train, current_prediction)\n",
    "    mse_errors_list.append(error)\n",
    "    \n",
    "    current_error = mean_squared_error(y_train, current_prediction)\n",
    "    mse_sklearn_errors_list.append(current_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.812550945781193"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_prediction = gbm_predict(X_test, base_algorithms_list, coefficients_list)\n",
    "mean_squared_error(y_test, current_prediction, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.160646605739455"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, current_prediction, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "\n",
    "Реализованный вами метод - градиентный бустинг над деревьями - очень популярен в машинном обучении. Он представлен как в самой библиотеке `sklearn`, так и в сторонней библиотеке `XGBoost`, которая имеет свой питоновский интерфейс. На практике `XGBoost` работает заметно лучше `GradientBoostingRegressor` из `sklearn`, но для этого задания вы можете использовать любую реализацию. \n",
    "\n",
    "Исследуйте, переобучается ли градиентный бустинг с ростом числа итераций (и подумайте, почему), а также с ростом глубины деревьев. На основе наблюдений выпишите через пробел номера правильных из приведенных ниже утверждений в порядке возрастания номера (это будет **ответ в п.4**):\n",
    "\n",
    "    1. С увеличением числа деревьев, начиная с некоторого момента, качество работы градиентного бустинга не меняется существенно.\n",
    "\n",
    "    2. С увеличением числа деревьев, начиная с некоторого момента, градиентный бустинг начинает переобучаться.\n",
    "\n",
    "    3. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга на тестовой выборке начинает ухудшаться.\n",
    "\n",
    "    4. С ростом глубины деревьев, начиная с некоторого момента, качество работы градиентного бустинга перестает существенно изменяться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5\n",
    "\n",
    "Сравните получаемое с помощью градиентного бустинга качество с качеством работы линейной регрессии. \n",
    "\n",
    "Для этого обучите `LinearRegression` из `sklearn.linear_model` (с параметрами по умолчанию) на обучающей выборке и оцените для прогнозов полученного алгоритма на тестовой выборке `RMSE`. Полученное качество - ответ в **пункте 5**. \n",
    "\n",
    "В данном примере качество работы простой модели должно было оказаться хуже, но не стоит забывать, что так бывает не всегда. В заданиях к этому курсу вы еще встретите пример обратной ситуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
